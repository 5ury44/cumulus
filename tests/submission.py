
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍
#################################################
# file to edit: CS4650_hw1_release_fa2025.ipynb͏︆͏󠄃͏󠄌͏󠄍͏︅͏︀͏︋͏︋͏󠄅͏︈͏︍

'''
surycuh@Vini distributed-checkpoint % export CUMULUS_LLM_HF_ID='gpt2'
python3 - <<'PY'
from submission import configure_cumulus, RemoteLLM, LLMGenerationConfig
configure_cumulus('http://localhost:8084')
llm = RemoteLLM(use_remote=True)
print(llm.generate('Answer in one word: The shape with 3 sides',
                   task_type='mcq')) 
PY

INFERENCE TESTING ON HF MODELS

'''

# DO NOT CHANGE THIS CELL
# Importing required libraries - DO NOT CHANGE THIS CELL

import os
import sys
import json
import pandas as pd
import datetime
from collections import Counter, defaultdict
import re
from dataclasses import dataclass
from typing import Callable, Dict, List, Sequence, Tuple, Optional
from tqdm import tqdm
import random
import numpy as np

import torch
import torch.nn.functional as F

# Graceful imports - if libraries aren't available, we'll handle it gracefully
try:
    from datasets import load_dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    print("Warning: datasets library not available")

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    print("Warning: transformers library not available")

try:
    from sentence_transformers import SentenceTransformer
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    print("Warning: sentence-transformers library not available")


# Cumulus Integration - Minimal changes for frictionless GPU execution
from typing import Optional

# Try to import Cumulus SDK - if not available, functions run locally
try:
    from cumulus.sdk import CumulusClient
    from cumulus.sdk.decorators import remote
    CUMULUS_AVAILABLE = True
except ImportError:
    CUMULUS_AVAILABLE = False
    print("Cumulus SDK not available - running locally")

# Global Cumulus client - can be configured via environment variable
_cumulus_client: Optional[CumulusClient] = None

def configure_cumulus(server_url: str = None, api_key: str = None):
    """
    Configure Cumulus client for remote GPU execution.
    
    Args:
        server_url: URL of the Cumulus worker server (e.g., "http://localhost:8080")
        api_key: Optional API key for authentication
    """
    global _cumulus_client
    if CUMULUS_AVAILABLE:
        server_url = server_url or os.getenv('CUMULUS_SERVER_URL', 'http://localhost:8080')
        _cumulus_client = CumulusClient(server_url, api_key)
        print(f"✅ Cumulus configured for remote execution: {server_url}")
    else:
        print("❌ Cumulus SDK not available - install with: pip install cumulus")

def use_remote_execution(enable: bool = True):
    """
    Enable or disable remote execution via Cumulus.
    
    Args:
        enable: If True, functions will run on remote GPU server
    """
    global _cumulus_client
    if enable and CUMULUS_AVAILABLE:
        if _cumulus_client is None:
            configure_cumulus()
        print("🚀 Remote execution enabled")
    else:
        _cumulus_client = None
        print("💻 Local execution enabled")

# Auto-configure if environment variable is set
if CUMULUS_AVAILABLE and os.getenv('CUMULUS_SERVER_URL'):
    configure_cumulus()


# Default model configuration (can be overridden via env)
DEFAULT_LLM_HF_ID = os.getenv('CUMULUS_LLM_HF_ID', 'gpt2')


# DO NOT CHANGE THIS CELL

# Import tokenizer for n-gram matching
try:
    import nltk
    nltk.download('punkt')
    nltk.download('punkt_tab')
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    print("Warning: nltk library not available")


# DO NOT CHANGE THIS CELL
# Defining global constants - DO NOT CHANGE THESE VALUES

RANDOM_SEED = 42
PADDING_VALUE = 0
UNK_VALUE     = 1
BATCH_SIZE = 128

torch.manual_seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)

working_dir = os.getcwd()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



# DO NOT CHANGE THIS CELL

@dataclass
class LLMGenerationConfig:
    """
    Configuration class for LLM generation parameters.
    This is for convenience for keeping track of the generation parameters.
    http://brainiac:9657/tree?token=9d0d4afb1bc218c5d85f6418e7d1e8007ef0dc1b2a44bd28
    Args:
        temperature (float): Controls randomness in sampling. Higher values make output more random.
        max_new_tokens (int): Maximum number of new tokens to generate.
    """
    temperature: float = 0.7
    max_new_tokens: int = 100


class LLM:
    """
    A wrapper class for Hugging Face language models that provides a unified interface
    for text generation, logit computation, and perplexity calculation.

    If transformers library is not available, falls back to deterministic stubs.
    """

    def __init__(self, hf_id: str = "gpt2", device: str = None, quantize: bool = True):
        """
        Initialize the LLM wrapper.

        Args:
            hf_id (str): Hugging Face model identifier
            device (str): Device to load model on ('cuda', 'cpu', 'mps')
        """
        self.hf_id = hf_id
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")
        
        if not TRANSFORMERS_AVAILABLE:
            print("Warning: transformers not available - LLM will use stub implementations")
            self.tokenizer = None
            self.model = None
            return

        # Use auto-loading with device_map="auto" for faster loading and automatic memory management
        self.tokenizer = AutoTokenizer.from_pretrained(hf_id)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        if device == 'cpu':
            self.model = AutoModelForCausalLM.from_pretrained(
                hf_id,
                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory
                load_in_8bit=False
            ).to(self.device).eval()
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                hf_id,
                device_map="auto",  # Automatically determine optimal device mapping
                torch_dtype=torch.float16,  # Use half precision for faster loading and less memory
                load_in_8bit=quantize,  # Enable 4-bit quantization for even more memory efficiency
            ).eval()


    @torch.inference_mode()
    def generate(self, prompt: str, task_type: str = "mcq", config: LLMGenerationConfig = None) -> str:
        """
        Generate text continuation for the given prompt using the underlying language model.

        This method takes a text prompt and generates additional text that continues from
        the prompt in a coherent manner. The generation process can be controlled through
        various parameters specified in the config object.

        The method tokenizes the input prompt, passes it through the model, and then
        decodes the generated token IDs back to text, excluding the original prompt tokens.

        Args:
            prompt (str): Input text prompt that the model will continue from
            config (LLMGenerationConfig): Configuration object containing generation parameters
                such as temperature, top_p, top_k, and max_new_tokens. If None, default
                parameters will be used.

        Returns:
            str: Generated text continuation without the original prompt. The text is
                stripped of leading/trailing whitespace and special tokens are removed
                during decoding.
        """
        if not TRANSFORMERS_AVAILABLE or self.model is None:
            # Return stub responses when transformers is not available
            if task_type == "mcq":
                return "A"  # Default MCQ answer
            elif task_type == "mt":
                return "This is a stub translation."
            elif task_type == "ssg":
                return "This is a stub story generation."
            else:
                return "Stub response"

        # set generation config
        assert task_type in ["mcq", "mt", "ssg"], "Invalid task_type. Must be one of: mcq, mt, ssg"

        if config is not None:
            # Use the provided config
            pass
        elif task_type == "mcq":
            config = LLMGenerationConfig(
                temperature=0.0,
                max_new_tokens=4,
            )
        elif task_type == "mt":
            config = LLMGenerationConfig(
                temperature=0.0,
                max_new_tokens=200, # max llama=189, qwen=188 on test set; mt contains longer sentences
            )
        elif task_type == "ssg":
            config = LLMGenerationConfig(
                temperature=0.7,
                max_new_tokens=100, # max llama=76, qwen=75 on all five sentences
                )



        # tokenize the prompt
        input_ids = self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)

        # generate the output
        if config.temperature > 0:
            output = self.model.generate(
                input_ids,
                do_sample=True,
                temperature=config.temperature,
                max_length=input_ids.shape[1] + config.max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
            )
        else:
            output = self.model.generate(
                input_ids,
                do_sample=False,
                max_length=input_ids.shape[1] + config.max_new_tokens,
                pad_token_id=self.tokenizer.eos_token_id,
            )

        generated = self.tokenizer.decode(
            output[0, input_ids.shape[1]:],
            skip_special_tokens=True
        )
        return generated.strip()

    @torch.inference_mode()
    def logits(self, prompt: str) -> torch.Tensor:
        """
        Get next-token logits for the given prompt.

        This method computes and returns the logits (raw, unnormalized prediction scores)
        for the next token that would follow the given prompt. These logits represent the model's
        prediction distribution over the entire vocabulary for the next token position.

        Args:
            prompt (str): Input text prompt for which to compute next-token predictions

        Returns:
            torch.Tensor: A tensor of shape (vocab_size,) containing the logits for each
                possible next token in the vocabulary. Higher values indicate tokens the
                model considers more likely to follow the prompt.
        """
        tokens = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.model(**tokens)
        # Return logits for the last token position


        #### YOUR CODE HERE ####

        return outputs.logits[0, -1, :]

        #### END YOUR CODE ####



    ### DO NOT CHANGE THIS FUNCTION ###
    @torch.inference_mode()
    def perplexity(self, text: str) -> float:
        """
        Calculate perplexity of the given text.

        Perplexity is a measurement of how well a probability model predicts a sample.
        Lower perplexity indicates the model is better at predicting the text.
        It is calculated as the exponentiated average negative log-likelihood of a sequence.

        Args:
            text (str): Input text for which to calculate perplexity
        Returns:
            float: Perplexity value
        """
        if self.model is None:
            return 100.0  # Fixed stub value

        encodings = self.tokenizer(text, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model(**encodings)
            logits = outputs.logits

        # Shift logits and labels for next-token prediction
        shift_logits = logits[:, :-1].contiguous()
        shift_labels = encodings.input_ids[:, 1:].contiguous()

        # Calculate cross-entropy loss
        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)
        loss = loss_fct(
            shift_logits.view(-1, shift_logits.size(-1)),
            shift_labels.view(-1)
        )

        return np.exp(loss.item())


# DO NOT CHANGE THIS CELL

class EmbeddingModel:
    """
    A wrapper around the Huggingface SentenceTransformer API for generating embeddings.
    This model creates semantic embeddings that can be used for measuring similarity
    between texts.
    """

    def __init__(self, hf_id: str = "sentence-transformers/all-MiniLM-L6-v2", dim: int = None):
        """
        Initialize the embedding model with the specified model ID.

        Args:
            hf_id (str): Hugging Face model ID for the SentenceTransformer model.
                         Default is "sentence-transformers/all-MiniLM-L6-v2".
            dim (int): Not used for SentenceTransformer models as the dimension is
                       determined by the model itself, but kept for API compatibility.
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            print("Warning: sentence-transformers not available - EmbeddingModel will use stub implementations")
            self.model = None
            self.dim = dim or 384  # Default dimension
            return
            
        self.model = SentenceTransformer(hf_id,  trust_remote_code=True)
        self.dim = self.model.get_sentence_embedding_dimension()

    def embed(self, text: str) -> torch.Tensor:
        """
        Create an embedding for the given text using the SentenceTransformer model.

        Args:
            text (str): Input text to embed. Can be of any length.

        Returns:
            torch.Tensor: Embedding vector representing the semantic content of the input text.
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE or self.model is None:
            # Return a random embedding as stub
            return torch.randn(self.dim)
            
        # SentenceTransformer returns numpy array, convert to torch tensor
        embedding = self.model.encode(text, convert_to_tensor=True)
        return embedding


# Remote execution wrappers for GPU-intensive functions
def _remote_llm_generate(prompt: str, task_type: str = "mcq", config_dict: dict = None, hf_id: str = "gpt2", device: str = None, quantize: bool = True):
    """Remote wrapper for LLM generation - GPU + quantized for big models."""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

    # Default config by task_type
    if config_dict is None:
        if task_type == "mcq":
            config_dict = {"temperature": 0.0, "max_new_tokens": 4}
        elif task_type == "mt":
            config_dict = {"temperature": 0.0, "max_new_tokens": 200}
        elif task_type == "ssg":
            config_dict = {"temperature": 0.7, "max_new_tokens": 100}
        else:
            config_dict = {"temperature": 0.7, "max_new_tokens": 100}

    temperature = float(config_dict.get("temperature", 0.7))
    max_new_tokens = int(config_dict.get("max_new_tokens", 100))

    # Load tokenizer/model (sentencepiece for Mistral/Llama)
    tokenizer = AutoTokenizer.from_pretrained(hf_id)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Prefer 4-bit quantization to fit 7B on 24GB VRAM
    bnb = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=torch.float16,
    )

    model = AutoModelForCausalLM.from_pretrained(
        hf_id,
        device_map="auto",
        quantization_config=bnb,
        torch_dtype=torch.float16,
    ).eval()

    # Prepare inputs
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=(temperature > 0.0),
            pad_token_id=tokenizer.eos_token_id,
        )

    generated_tokens = outputs[0][inputs.input_ids.shape[1]:]
    return tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()

def _remote_llm_logits(prompt: str, hf_id: str = "gpt2", device: str = None, quantize: bool = True):
    """Remote wrapper for logits - load quantized to avoid OOM on big models."""
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    tok = AutoTokenizer.from_pretrained(hf_id)
    if tok.pad_token is None: tok.pad_token = tok.eos_token
    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)
    model = AutoModelForCausalLM.from_pretrained(hf_id, device_map="auto", quantization_config=bnb, torch_dtype=torch.float16).eval()
    tokens = tok(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad(): out = model(**tokens)
    return out.logits[0, -1, :]

def _remote_llm_perplexity(text: str, hf_id: str = "gpt2", device: str = None, quantize: bool = True):
    """Remote wrapper for perplexity - load quantized to avoid OOM on big models."""
    import torch
    import numpy as np
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    tok = AutoTokenizer.from_pretrained(hf_id)
    if tok.pad_token is None: tok.pad_token = tok.eos_token
    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type="nf4", bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.float16)
    model = AutoModelForCausalLM.from_pretrained(hf_id, device_map="auto", quantization_config=bnb, torch_dtype=torch.float16).eval()
    enc = tok(text, return_tensors="pt").to(model.device)
    with torch.no_grad(): logits = model(**enc).logits
    shift_logits = logits[:, :-1].contiguous()
    shift_labels = enc.input_ids[:, 1:].contiguous()
    loss = torch.nn.functional.cross_entropy(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    return float(torch.exp(loss).item())

def _remote_embedding_similarity(generation: str, reference: str, hf_id: str = "sentence-transformers/all-MiniLM-L6-v2"):
    """Remote wrapper for embedding similarity computation - runs on GPU server."""
    embedder = EmbeddingModel(hf_id)
    return compute_embedding_similarity(generation, reference, embedder)

def _remote_embedding_diversity(generations: List[str], hf_id: str = "sentence-transformers/all-MiniLM-L6-v2"):
    """Remote wrapper for embedding diversity computation - runs on GPU server."""
    embedder = EmbeddingModel(hf_id)
    return compute_embedding_diversity(generations, embedder)

def _remote_coherence(sentences: List[str], hf_id: str = "sentence-transformers/all-MiniLM-L6-v2"):
    """Remote wrapper for coherence computation - runs on GPU server."""
    model = EmbeddingModel(hf_id)
    return compute_coherence(sentences, model)

# Enhanced LLM class with optional remote execution
class RemoteLLM(LLM):
    """
    Enhanced LLM class that can optionally run computations on remote GPU server.
    """
    
    def __init__(self, hf_id: Optional[str] = None, device: str = None, quantize: bool = True, use_remote: bool = None):
        """
        Initialize LLM with optional remote execution.
        
        Args:
            hf_id: Hugging Face model identifier
            device: Device to load model on
            quantize: Whether to use quantization
            use_remote: If True, use remote execution. If None, auto-detect based on Cumulus availability
        """
        self.use_remote = use_remote if use_remote is not None else (_cumulus_client is not None)
        self.hf_id = hf_id or DEFAULT_LLM_HF_ID
        self.device = device
        self.quantize = quantize
        
        if not self.use_remote:
            super().__init__(hf_id, device, quantize)
    
    def generate(self, prompt: str, task_type: str = "mcq", config: LLMGenerationConfig = None) -> str:
        """Generate text with optional remote execution."""
        if self.use_remote and _cumulus_client:
            # Convert config to dict for serialization
            config_dict = config.__dict__ if config else None
            return _cumulus_client.run(
                func=_remote_llm_generate,
                gpu_memory=0.98,
                duration=3600,
                requirements=["torch", "transformers", "accelerate", "bitsandbytes", "sentencepiece", "safetensors"],
                prompt=prompt,
                task_type=task_type,
                config_dict=config_dict,
                hf_id=self.hf_id,
                device=self.device,
                quantize=self.quantize
            )
        else:
            return super().generate(prompt, task_type, config)
    
    def logits(self, prompt: str) -> torch.Tensor:
        """Compute logits with optional remote execution."""
        if self.use_remote and _cumulus_client:
            return _cumulus_client.run(
                func=_remote_llm_logits,
                gpu_memory=0.8,
                duration=3600,
                requirements=["torch", "transformers", "accelerate", "bitsandbytes", "sentencepiece", "safetensors"],
                prompt=prompt,
                hf_id=self.hf_id,
                device=self.device,
                quantize=self.quantize
            )
        else:
            return super().logits(prompt)
    
    def perplexity(self, text: str) -> float:
        """Compute perplexity with optional remote execution."""
        if self.use_remote and _cumulus_client:
            return _cumulus_client.run(
                func=_remote_llm_perplexity,
                gpu_memory=0.8,
                duration=3600,
                requirements=["torch", "transformers", "accelerate", "bitsandbytes", "sentencepiece", "safetensors"],
                text=text,
                hf_id=self.hf_id,
                device=self.device,
                quantize=self.quantize
            )
        else:
            return super().perplexity(text)

# Enhanced EmbeddingModel class with optional remote execution
class RemoteEmbeddingModel(EmbeddingModel):
    """
    Enhanced EmbeddingModel class that can optionally run computations on remote GPU server.
    """
    
    def __init__(self, hf_id: str = "sentence-transformers/all-MiniLM-L6-v2", dim: int = None, use_remote: bool = None):
        """
        Initialize EmbeddingModel with optional remote execution.
        
        Args:
            hf_id: Hugging Face model identifier
            dim: Model dimension (unused for SentenceTransformer)
            use_remote: If True, use remote execution. If None, auto-detect based on Cumulus availability
        """
        self.use_remote = use_remote if use_remote is not None else (_cumulus_client is not None)
        self.hf_id = hf_id
        
        if not self.use_remote:
            super().__init__(hf_id, dim)
    
    def embed(self, text: str) -> torch.Tensor:
        """Create embedding with optional remote execution."""
        if self.use_remote and _cumulus_client:
            return _cumulus_client.run(
                func=lambda t, model_id: EmbeddingModel(model_id).embed(t),
                gpu_memory=0.6,
                duration=1800,
                requirements=["torch", "sentence-transformers"],
                t=text,
                model_id=self.hf_id
            )
        else:
            return super().embed(text)


### YOUR CODE HERE ###

_mcq_regex = re.compile(r'(?i)\b([ABCD])\b')

### END YOUR CODE ###

def compute_regex_accuracy(generation: str, reference: str) -> tuple[float, str]:
    """
    Extract the first letter A-D from the generation and compare with reference.

    Args:
        generation (str): Generated text from the model
        reference (str): Ground truth answer (A, B, C, or D)

    Returns:
        float: 1.0 if correct, 0.0 if incorrect
        str: The predicted letter (A, B, C, or D)
    """
    match = _mcq_regex.search(generation)
    regex_pred = match.group(1).upper() if match else None
    return float(regex_pred == reference.strip().upper()), regex_pred

LETTERS = ["A", "B", "C", "D"]

def compute_logit_accuracy(logits: torch.Tensor, reference: str, model: LLM, valid_letters: List[str] = None) -> Tuple[float, str]:
    """
    Compute accuracy based on logits for the first token generated by the LLM.

    Args:
        logits (torch.Tensor): Logits from the model for the first token
        reference (str): Ground truth answer (A, B, C, or D)
        model (LLM): The language model used for generation, needed for tokenizer access
        valid_letters (List[str]): List of valid letters to restrict argmax to (e.g., ['A', 'B', 'C', 'D'])
                                If None, uses the entire vocabulary

    Returns:
        float: 1.0 if the token with highest logit matches reference, 0.0 otherwise
        str: The predicted letter (If `valid_letters` is None, then the predicted letter is not constrained to be one of the valid letters, but can be any letter in the vocabulary)
    """

    ### YOUR CODE HERE ###

    if valid_letters is not None:
        im = {}
        for lt in valid_letters:
            tk = model.tokenizer.encode(lt, add_special_tokens=False)
            im[lt] = tk[0]

        rl = {}
        for lt, ti in im.items():
            rl[lt] = logits[ti].item()

        pl = max(rl, key=rl.get)
    else:
        pt = torch.argmax(logits).item()
        pl = model.tokenizer.decode([pt]).strip()

    return float(pl.upper() == reference.strip().upper()), pl


    ### END YOUR CODE ###


# DO NOT CHANGE THIS CELL

def preprocess_text(text: str) -> List[str]:
    """
    Tokenize a text string into a list of tokens. This is only for n-gram matching.

    Args:
        text (str): The input text string to tokenize

    Returns:
        List[str]: A list of tokens extracted from the input text
    """
    return nltk.word_tokenize(text)


def _ngrams(tokens: Sequence[str], n: int) -> List[Tuple[str, ...]]:
    """
    Extract n-grams from a sequence of tokens.

    Args:
        tokens (Sequence[str]): A sequence of tokens (words, characters, etc.)
        n (int): The size of each n-gram

    Returns:
        List[Tuple[str, ...]]: A list of tuples, where each tuple contains n consecutive tokens
                              from the input sequence
    """
    ### YOUR CODE HERE ###
    if n <= 0 or len(tokens) < n:
        return []
    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]
    ### END YOUR CODE ###


def compute_corpus_ngram_overlap(candidates: List[str], references: List[str], n: int = 2) -> float:
    """
    Compute corpus-level n-gram overlap by aggregating across all sentences.
    Compute the n-gram overlap, accounting for the number of times each n-gram appears in the candidate and reference.

    Args:
        candidates (List[str]): List of candidate translations
        references (List[str]): List of reference translations
        n (int): N-gram order

    Returns:
        float: Corpus-level n-gram overlap score
    """
    ### YOUR CODE HERE ###
    th = 0
    tr = 0
    for i, j in zip(candidates, references):
        hn = _ngrams(preprocess_text(i), n)
        rn = _ngrams(preprocess_text(j), n)
        if not rn:
            continue
        else:
            hc = Counter(hn)
            rc = Counter(rn)
            th += sum((hc & rc).values())
            tr += sum(rc.values())

    if not tr:
        return 0.0
    return (th / tr)

    ### END YOUR CODE ###


def compute_corpus_bleu(candidates: List[str], references: List[str], max_n: int = 4) -> float:
    """
    Compute corpus-level BLEU score by aggregating n-gram statistics across all sentences.

    Args:
        candidates (List[str]): List of candidate translations
        references (List[str]): List of reference translations
        max_n (int): Maximum n-gram order to consider

    Returns:
        float: Corpus-level BLEU score
    """
    # Aggregate statistics across all sentences
    total_cand_len = 0
    total_ref_len = 0
    ngram_matches = [0] * max_n
    ngram_totals = [0] * max_n

    # iterate over each candidate and reference
    for cand, ref in zip(candidates, references):
        cand_tokens = preprocess_text(cand)
        ref_tokens = preprocess_text(ref)

        total_cand_len += len(cand_tokens)
        total_ref_len += len(ref_tokens)

        for n in range(1, max_n + 1):
            cand_ngrams = Counter(_ngrams(cand_tokens, n))
            ref_ngrams = Counter(_ngrams(ref_tokens, n))

            overlap = {ng: min(count, ref_ngrams[ng]) for ng, count in cand_ngrams.items()}
            ngram_matches[n-1] += sum(overlap.values())
            ngram_totals[n-1] += sum(cand_ngrams.values())

    precisions = []

    ### YOUR CODE HERE ###

    # calculate precision for each n-gram order
    precisions = []
    for i in range(max_n):
        total = ngram_totals[i]
        if total > 0:
            precisions.append(ngram_matches[i] / total)
        else:
            precisions.append(0.0)

    ### END YOUR CODE ###

    # Geometric mean with smoothing
    eps = 1e-9
    geo_mean = np.exp(sum(np.log(p + eps) for p in precisions) / max_n)

    # Brevity penalty
    bp = 1.0 if total_cand_len > total_ref_len else np.exp(1 - total_ref_len / max(total_cand_len, 1))

    return bp * geo_mean


def compute_embedding_similarity(
    generation: str,
    reference: str,
    embedder: EmbeddingModel = None
) -> float:
    """
    This function computes the semantic similarity between a generated text and a reference text
    using embeddings and cosine similarity.

    Embedding-based similarity captures semantic meaning beyond exact word matches, allowing
    for evaluation of paraphrases and texts that convey similar meaning with different words.

    Args:
        generation (str): Generated text to be evaluated
        reference (str): Reference text to compare against
        embedder (EmbeddingModel): Model to create text embeddings. If None, a default model is used.

    Returns:
        float: Cosine similarity between the embeddings, ranging from -1.0 to 1.0,
              where higher values indicate greater semantic similarity
    """
    ### YOUR CODE HERE ###

    if embedder is None:
        embedder = labse

    if isinstance(generation, str):
        gens = [generation]
    else:
        gens =list(generation)
    if isinstance(reference, str):
        refs = [reference]
    else:
        refs=list(reference)

    if not gens or not refs:
        return 0.0

    sims: list[float] = []
    for gen_text, ref_text in zip(gens, refs):

        g = F.normalize(torch.as_tensor(embedder.embed(gen_text), dtype=torch.float32), p=2, dim=0, eps=1e-9)
        r = F.normalize(torch.as_tensor(embedder.embed(ref_text), dtype=torch.float32), p=2, dim=0, eps=1e-9)
        sims.append(float(torch.dot(g, r).item()))

    if not sims:
        return 0.0
    ret = float(sum(sims) / len(sims))
    return ret

    ### END YOUR CODE ###


def compute_perplexity(text: str, big_llm: LLM = None) -> float:
    """
    This function computes the perplexity of a given text using a language model.

    Perplexity is a measurement of how well a probability model predicts a sample.
    Lower perplexity indicates the language model is better at predicting the text.

    Args:
        text (str): The generated text to evaluate. This should be a coherent piece
                   of text that we want to measure the perplexity of.
        big_llm (LLM): A language model instance used for perplexity computation.
                      If None is provided, a default LLM instance will be created.

    Returns:
        float: The perplexity value of the input text. Lower values indicate the text
              is more predictable according to the language model.
    """
    ### YOUR CODE HERE ###
    if big_llm is None:
        big_llm = llama

    tokens = big_llm.tokenizer(text, return_tensors="pt")
    tokens = {k: v.to(big_llm.device) for k, v in tokens.items()}

    if tokens["input_ids"].size(1) < 2:
        return float("inf")

    with torch.inference_mode():
        s_logits = big_llm.model(**tokens).logits[:, :-1, :].contiguous()
        s_labels = tokens["input_ids"][:, 1:].contiguous()
        loss = F.cross_entropy(
            s_logits.view(-1, s_logits.size(-1)),
            s_labels.view(-1),
            reduction="mean",
        )

    return float(torch.exp(loss).item())
    ### END YOUR CODE ###


def compute_ngram_diversity(text: str, n: int = 2) -> float:
    """
    This function computes the distinct n-gram ratio, which is a measure of text diversity.

    The distinct n-gram ratio is calculated by dividing the number of unique n-grams
    by the total number of n-grams in the text. A higher ratio indicates more diverse text.

    Args:
        text (str): The generated text to evaluate
        n (int): The n-gram order (default: 2 for bigrams)

    Returns:
        float: The ratio of unique n-grams to total n-grams, ranging from 0.0 to 1.0
              where 1.0 means all n-grams are unique
    """

    ### YOUR CODE HERE ###
    t = preprocess_text(text)
    if n <= 0 or len(t) < n:
        return 0.0
    n = _ngrams(t, n)
    if not n:
        return 0.0
    return float(len(Counter(n)) / len(n))

    ### END YOUR CODE ###

# DO NOT CHANGE THIS CELL

def compute_embedding_diversity(
    generations: List[str],
    embedder: EmbeddingModel
) -> float:
    """
    This function computes the diversity of multiple generated texts using embeddings.

    The diversity score is calculated by embedding each generation, computing the cosine
    similarity between the first generation (used as reference) and all other generations,
    and then returning 1 minus the mean similarity. A higher score indicates more diverse
    generations.

    Args:
        generations (List[str]): A list of generated texts to evaluate for diversity
        embedder (EmbeddingModel): The embedding model to use for converting text to vectors.
                                  If None, a default EmbeddingModel will be instantiated.

    Returns:
        float: Diversity score ranging from 0.0 to 1.0, where higher values indicate
              more diverse generations. Returns 0.0 if fewer than 2 generations are provided.
    """
    ### YOUR CODE HERE ###
    if not generations or len(generations) < 2:
        return 0.0

    em = [F.normalize(e, p=2, dim=0, eps=1e-9) for e in [torch.as_tensor(embedder.embed(t), dtype=torch.float32) for t in generations]]

    s = torch.mv(torch.stack(em[1:], dim=0), em[0])

    if s.numel() > 0:
        ms = s.mean().item()
    else:
        0.0
    return float(1.0 - ms)
    ### END YOUR CODE ###


def split_sentences(text):
    """
    Split text at punctuation marks: ., !, ?

    Args:
        text (str): Input text to split into sentences

    Returns:
        List[str]: List of sentences with whitespace stripped
    """
    if not isinstance(text, str):
        raise ValueError(f"Expected string input, got {type(text)}")

    sentences = re.split(r'[.!?]', text)
    return [s.strip() for s in sentences if s.strip()]


def compute_coherence(sentences: List[str], model: EmbeddingModel) -> float:
    """
    This function computes the coherence of a text based on the semantic similarity
    between adjacent sentences.

    Coherence measures how well the sentences in a text connect to each other in a logical
    and consistent way. Higher coherence indicates a more natural flow between sentences.

    Args:
        sentences (List[str]): A list of sentences to evaluate for coherence
        model (EmbeddingModel): The embedding model to use for computing text embeddings

    Returns:
        float: Coherence score. Higher values indicate better coherence between
               adjacent sentences. Returns 0.0 if fewer than 2 sentences are provided.
    """

    #### YOUR CODE HERE ####

    if len(sentences) < 2:
        return 0.0

    e = [F.normalize(v, p=2, dim=0, eps=1e-9) for v in [torch.as_tensor(model.embed(s), dtype=torch.float32) for s in sentences]]

    s = (torch.stack(e, dim=0)[:-1] * torch.stack(e, dim=0)[1:]).sum(dim=-1)
    if s.numel() > 0:
        return float(s.mean().item())
    return 0.0

    #### END YOUR CODE ####


# Usage Examples for Cumulus Integration
def example_usage():
    """
    Example showing how to use Cumulus for remote GPU execution.
    """
    print("=== Cumulus Integration Examples ===")
    
    # Check what's available
    print(f"📦 Libraries available:")
    print(f"  - Transformers: {'✅' if TRANSFORMERS_AVAILABLE else '❌'}")
    print(f"  - Sentence Transformers: {'✅' if SENTENCE_TRANSFORMERS_AVAILABLE else '❌'}")
    print(f"  - NLTK: {'✅' if NLTK_AVAILABLE else '❌'}")
    print(f"  - Cumulus: {'✅' if CUMULUS_AVAILABLE else '❌'}")
    
    if not TRANSFORMERS_AVAILABLE or not SENTENCE_TRANSFORMERS_AVAILABLE:
        print("\n💡 To install missing ML libraries:")
        print("   pip install transformers sentence-transformers nltk datasets")
    
    if not CUMULUS_AVAILABLE:
        print("\n💡 To install Cumulus SDK:")
        print("   pip install cumulus")
    
    # Method 1: Configure Cumulus globally
    configure_cumulus("http://localhost:8080")  # or your server URL
    use_remote_execution(True)
    
    # Method 2: Use enhanced classes directly
    llm = RemoteLLM("gpt2", use_remote=True)
    embedding_model = RemoteEmbeddingModel(use_remote=True)
    
    # Method 3: Environment variable configuration
    # Set CUMULUS_SERVER_URL=http://your-server:8080
    # Then just use RemoteLLM() and RemoteEmbeddingModel() - they auto-detect
    
    print("\n✅ Examples configured - ready for remote GPU execution!")
    print("\n🚀 Usage:")
    print("   llm = RemoteLLM('gpt2', use_remote=True)")
    print("   result = llm.generate('Hello world', task_type='mcq')")
    print("   print(result)  # Will run on remote GPU server!")

# Auto-run example if this file is executed directly
if __name__ == "__main__":
    example_usage()