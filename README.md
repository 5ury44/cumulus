# Cumulus - Cumulus-Based Distributed Execution SDK

A distributed execution system that sends code to remote GPU servers with Cumulus GPU partitioning, similar to Modal but without Docker.

## ğŸ—ï¸ Architecture

```
[Local Client] â†’ [Code Packaging] â†’ [Remote Server] â†’ [Cumulus Partition] â†’ [Execution] â†’ [Results]
```

### Components

1. **Client SDK** (`sdk/`) - Local Python SDK for wrapping and sending code
2. **Server Worker** (`worker/`) - Remote execution worker with Cumulus integration
3. **Code Transfer** - ZIP-based code packaging and transfer
4. **Cumulus Integration** - GPU partitioning and resource management

## ğŸš€ Quick Start

### 1. Setup Remote Server

```bash
# On your GPU server
cd /path/to/chronos
./install-chronos-gpu.sh

# Start the worker
python3 cumulus/worker/server.py --host 0.0.0.0 --port 8080
```

### 2. Use Local SDK

```python
from cumulus.sdk import CumulusClient

# Create client
client = CumulusClient(server_url="http://your-server:8080")

# Define your function
def train_model():
    import torch
    model = torch.nn.Linear(10, 1)
    # Your training code here
    return model.state_dict()

# Run on remote GPU with Cumulus partition
result = client.run(
    func=train_model,
    gpu_memory=0.5,  # 50% of GPU memory
    duration=3600,   # 1 hour
    requirements=["torch", "numpy"]
)

print(f"Training result: {result}")
```

## ğŸ“ Project Structure

```
cumulus/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ sdk/                      # Client SDK
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ client.py            # Main client class
â”‚   â”œâ”€â”€ code_packager.py     # Code packaging utilities
â”‚   â””â”€â”€ decorators.py        # Function decorators
â”œâ”€â”€ worker/                   # Server-side worker
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ server.py            # FastAPI server
â”‚   â”œâ”€â”€ executor.py          # Code execution engine
â”‚   â”œâ”€â”€ chronos_manager.py   # Cumulus integration
â”‚   â””â”€â”€ requirements.txt     # Server dependencies
â”œâ”€â”€ examples/                 # Usage examples
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ gpu_training.py
â”‚   â””â”€â”€ distributed_training.py
â””â”€â”€ tests/                    # Test suite
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_installation.py  # Local SDK installation test
    â”œâ”€â”€ test_remote.py        # Remote server connection test
    â”œâ”€â”€ test_ssh.py           # Server-side execution test
    â”œâ”€â”€ test_simple_pytorch.py # PyTorch CUDA functionality test
    â””â”€â”€ test_gpu_benchmark.py # Comprehensive GPU performance test
```

## ğŸ”§ Features

- **No Docker Required** - Direct Python execution on remote server
- **Cumulus Integration** - Automatic GPU partitioning and resource management
- **Code Packaging** - Automatic dependency detection and packaging
- **Result Serialization** - Automatic serialization of return values
- **Error Handling** - Comprehensive error reporting and logging
- **Resource Management** - Automatic cleanup and resource monitoring

## ğŸ“Š Benefits over Docker-based Solutions

1. **Faster Startup** - No container build/start time
2. **Lower Resource Overhead** - Direct Python execution
3. **Better GPU Access** - Direct Cumulus integration
4. **Simpler Debugging** - Direct access to server logs
5. **Dynamic Dependencies** - Install packages on-demand

## ğŸ§ª Testing

### Local Testing

```bash
# Test the SDK installation
python tests/test_installation.py

# Test with remote server (requires running worker)
python tests/test_remote.py
```

### Server Testing

```bash
# Test on the server directly
python tests/test_ssh.py

# Test PyTorch CUDA functionality
python tests/test_simple_pytorch.py

# Run comprehensive GPU benchmarks
python tests/test_gpu_benchmark.py
```

## ğŸ¯ Use Cases

- **Machine Learning Training** - Send training scripts to GPU servers
- **Data Processing** - Run compute-intensive tasks remotely
- **Research Experiments** - Execute experiments on shared GPU resources
- **Development** - Test code on remote hardware without local setup
